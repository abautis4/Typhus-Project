# University of Houston - Clinical Decision Support System (UHCDSS)
In this repository, you will find the necessary tools to use the Clinical Decision Support System (CDSS) developed in the Computational Biomedicine Laboratory (CBL) of the University of Houston.

# Authors

These notebooks are produced by Abraham Bautista-Castillo (https://uh.edu/cbl/people/abraham-castillo.php) and Ioannis A. Kakadiaris (https://uh.edu/cbl/people/about-director.php).

# License and copyright
All material belongs to the University of Houston (https://www.uh.edu/).

All computer code is released under the University of Houston license.

Permission is at this moment granted, free of charge, till 8/31/2023 to the Methodist Hospital Personnel working towards the Pilot AIM-AHEAD to use the Software without limitation the rights to use, copy, modify, and merge the Software.

After 8/31/2023 all code derived from this code must be deleted and email send to Prof. Kakadiaris (ioannisk@uh.edu)

The above copyright and permission notice shall be included in all copies or derivatives of the Software.
 
Using this code should reference the publication associated with this work. Reference is available upon request to Prof. Kakadiaris

Any distribution of this Software and associated documentation files must be previously discussed with Prof. Kakadiaris and receive express written authorization.

THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

# Code folder
The Code folder contains the Jupyter notebooks and python functions to carry out the steps necessary to use the CDSS. To execute each stage that makes up the CDSS, it is necessary to open the Code folder and select the Jupyter Notebook with the stage's name to run. The execution order of these Jupyter Notebooks is as follows:

- UHCDSS-pre-processing.ipynb
- UHCDSS-training-importance.ipynb
- UHCDSS-classification.ipynb

This folder also contains the following python functions necessary to run the notebooks:

- functions.py
- UHCDSS-pre-processing.py
- UHCDSS-training-importance.py
- UHCDSS-classification.py

And a .text file that contains the required packages to run all the notebooks:
- UHCDSS-requirements.txt

The last file contains the commands to install the packages needed to run the CDSS tool. To run the installation command, it is only necessary to execute the first code cell of any of the Jupyter notebooks. The requirements installation cell looks like this:

```
pip install -r requirements.txt
```
<mark>This only needs to be done once.</mark> After running this cell once on any Jupyter Notebook, it is unnecessary to rerun it for that computer.

<mark>PLEASE, DO NOT MOVE OR EDIT THESE FILES.</mark> Any change in these files could prevent the CDSS from working correctly.

# Data folder
The Data folder is where <mark>the dataset must be stored</mark> so the Jupyter UHCDSS-pre-processing.ipynb notebook can use it. In this folder will also be stored the files processed_dataset.csv and UH-att_module_scores.csv, generated by the Jupyter notebooks UHCDSS-pre-processing.ipynb and UHCDSS-training-importance.ipynb, respectively.
# Models folder
The Models folder contains the models generated by the UHCDSS-training-importance.ipynb Jupyter notebook. These models are stored with the extension ".keras" and are used by the Jupyter notebook UCHDSS-classification.ipynb
# Environment folder
In the Environment folder, there are three files with the extension ".txt":

- UHCDSS-pre-processing-environment.txt
- UHCDSS-training-importance-environment.txt
- UHCDSS-classification-environment.txt

These ".txt" files contain the modifiable variables of each stage necessary to use the tool. <mark> THESE ARE THE ONLY FILES THAT NEED TO BE MODIFIED FOR THE USE OF THE CDSS TOOL.</mark>

# HOW TO RUN IT?

## Pre-processing Stage

- The first step is to store the dataset to be processed in the Data folder. This file can be an Excel file or a CSV file.
- Second, you need to open the "UHCDSS-pre-processing-environment.txt" file in the Environment folder. Here the file name and label values will be modified. The dataset file name should carry the file extension, i.e., xls or xlsx if it is an Excel or CSV file. This name must be enclosed between the characters ' ', with no spaces. The "label" variable must be modified by the name of the label or objective column of the dataset. If the objective is to identify patients with Typhus and the column that identifies the patients is called Typhus, the variable must be identified as 'Typhus':
```datafolder = 'Data'
filename = 'name_of_the_dataset.csv'
label = 'Typhus'
using_thresholds = 'No'
thresholds = 'None'
create_new_column = 'No'
new_column_name = 'None'
new_column_operation = "None"
data_imputation = 'No'
imputation_columns = 'None'
normalize = 'Yes'
columns_to_normalize = 'All'
name_of_processed_file = 'processed_dataset.csv'
```
- Then go to the Code folder, where they will open the UHCDSS-pre-processing.ipynb notebook. You will execute the cell to install the necessary packages for the CDSS tool. Once the installation is finished, you will execute the cell that performs the pre-processing stage of the CDSS tool:
```
run UHCDSS-pre-processing.py
```
This will generate the "processed_dataset.csv" file stored in the Data folder, concluding with the pre-processing stage.

## Training and Importance Stage

- Open the "UHCDSS-training-importance-environment.txt" file (in Environment folder) where the label variable will be modified again with the column's name corresponding to the target or label, in this case, Typhus. Other adjustable parameters for training the deep learning neural model are the loss function, the optimizer to use during training, and the ratio in which the pre-processed dataset will be split between training/testing. <mark>We do not recommend changing any of these parameters.</mark>
```
filename = 'processed_dataset.csv'
datafolder = 'Data'
label = 'Typhus'
loss_function = 'mse'
optimizer = 'adam'
test_size = 0.5
models_folder = 'Models'
path_checkpoint1 = 'UH-LSTM-model-A.keras'
path_checkpoint2 = 'UH-LSTM-model-B.keras'
att_module_scores = 'UH-att_module_scores'
```
- Go to the Code folder again and open the "UHCDSS-training-importance.ipynb" Jupyter notebook, where this time you will only execute the command line of the notebook to run the training stage:
```
run UHCDSS-training-importance.py
```
This generates three files, the trained models:


    - UH-LSTM-model-A.keras
    - UH-LSTM-model-B.keras

Which will be stored in the Models folder. And the file with the attention/relevance values of each feature used during the deep learning model training:

- UH-att_module_scores.csv

Which will be stored in the Data folder.

## Classification Stage
- For this final stage, the .txt file "UHCDSS-classification-environment.txt" will be opened. The filename variable will be modified again with the name and extension of the dataset ("processed_dataset.csv" or any other dataset) and the variable corresponding to the label (Typhus). In addition, the name and extension of the previously trained model to be used will be specified in the variable 'model':
```
filename = 'processed_dataset.csv'
datafolder = 'Data'
label = 'Typhus'
models_folder = 'Models'
model = 'UH-LSTM-model-A.keras'
```
- Finally, in the Code folder, open the "UHCDSS-Classification.ipynb" notebook and run the last command cell that looks like this:
```
run UHCDSS-classification.py
```
This will generate a result message with the confusion matrix and the precision of the classification made with the model used:
```
WARNING:tensorflow:Layer LSTM will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
Model .... created !
Model .... loaded !
Prediction .... DONE ! 

Confusion Matrix: 
[[133   0]
 [  0  87]]

 Accuracy: 1.0
